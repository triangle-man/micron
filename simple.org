#+title: The simplest logic ever
#+startup: showall
#+options: toc:nil
#+latex_header: \usepackage{ebproof}
#+latex_header: \DeclareMathOperator{\unit}{\text{\tt unit}}
#+latex_header: \DeclareMathOperator{\id}{\text{\tt id}}
#+latex_header: \DeclareMathOperator{\fst}{\text{\tt fst}}
#+latex_header: \DeclareMathOperator{\snd}{\text{\tt snd}}
#+latex_header: \DeclareMathOperator{\dom}{dom}
#+latex_header: \DeclareMathOperator{\rng}{rng}

* Introduction

The subject of formal logic is frankly confusing to me. There are multiple kinds
of logic (classical, intuitionistic, ...) and multiple approaches to the
formalism (‘Hilbert style,’ ‘natural deduction,’ ‘sequent calculus’). References
in the subject are full of phrases like ‘Unfortunately, the computational
significance is somewhat obscured by syntactic complications that, although
certainly immaterial, have never really been overcome,’[fn:1] which are not
reassuring. 

It is asserted that intuisionistic logic is the same as a typed lambda calculus
and that both are really some kind of category. If true, that would suggest a
way into this subject: observe that it is a category (whatever it is) and then
allow category theory to provide the organising concepts. I have not seen this
done (for logic), at least, not in any way I understand, so I thought I would
have a go at running the whole subject for some trivial subpart of
logic. Specifically, that ‘fragment’ (as they say) which includes only
propositions built from $\wedge$ (/conjunction/, meaning ‘and’) and $\vee$
(/disjunction/, meaning ‘or.’)

* Natural deduction, intuitionistic logic

There is given a set of /propositions/, constructed as follows:

$A$, $B$, $C$, $\dotsc$ are /atomic propositions/, where the symbols come from
some countable alphabet. $\top$ (‘true’, ‘top’) and $\bot$ (‘false’, ‘absurity’,
‘bottom’) are atomic propositions.

Atomic propositions are propositions. For any two propositions, $\phi$ and
$\psi$, the formulae $(\phi\wedge\psi)$ and $(\phi\vee\psi)$ are
propositions. We use $P$, $Q$, $R$, $\dotsc$, and $\phi$, $\psi$, $\dotsc$ to
denote propositions.

The interpretation is different in classical and intuitionistic logic. In
classical logic, to assert the proposition $A$ is to assert that whatever is
meant by $A$ is /true/. A model for a classical logic assigns to each
proposition the set $\{\text{true}, \text{false}\}$. In intuitionistic logic, to
assert $A$ is to say that one has a /proof/ of $A$. 

Give some collection of propositions it may be that some of them are true (or,
at any rate, that we have a proof of them). 

Here are the rules that are used to construct proofs. One imagines writing the
premises of the argument at the top, then drawing a horizontal line where the
rules say one may make a deduction, them writing the conclusion below the
line. Lines are typically labelled by the rule that one used to draw the
conclusion.

** The rules of conjunction

\begin{equation}
  \begin{prooftree}
    \hypo{P}
    \hypo{Q}
    \infer2[$\wedge_{\text{I}}$]{P \wedge Q}
  \end{prooftree}
\end{equation}
and
\begin{equation}
  \begin{prooftree}
    \hypo{P \wedge Q}
    \infer1[$\wedge_{\text{E1}}$]{P}
  \end{prooftree}
  \qquad\text{and}\qquad
  \begin{prooftree}
    \hypo{P \wedge Q}
    \infer1[$\wedge_{\text{E2}}$]{Q}
  \end{prooftree}.
\end{equation}

The intended interpration of the first rule, known as ‘conjunction
introduction,’ is something like, ‘from (a proof of) $P$ and (a proof of) $Q$
one may obtain (a proof of) $P∧Q$. The second pair of rules, ‘conjunction
elimination,’ say the converse: given $P∧Q$, one may obtain either $P$ or $Q$ or
both.

** The rules of disjunction

\begin{equation}
  \begin{prooftree}
    \hypo{P}
    \infer1[$\vee_{\text{I1}}$]{P \vee Q}
  \end{prooftree}
  \qquad\text{and}\qquad
  \begin{prooftree}
    \hypo{Q}
    \infer1[$\vee_{\text{I2}}$]{P \vee Q}
  \end{prooftree}
\end{equation}
and
\begin{equation}
  \begin{prooftree}
    \hypo{P \vee Q}
    \hypo{[P]}
    \ellipsis{}{R}
    \hypo{[Q]}
    \ellipsis{}{R}
    \infer3[$\vee_{\text{E}}$]{R}
  \end{prooftree}.
\end{equation}

There would be a pleasing duality here, were it not for the strange construction
that is the rule labelled $\vee_\text{E}$, disjunction elimination. First of
all, there is some new notation: the ellipsis $[P]̣\dotsb R$, say, means that
there is to be inserted some proof of $R$, starting from the premise $P$. The
square brackets around the premises are introduced when we apply the rule
$\vee_\text{E}$ and mean that these premises are no longer to be considered
premises of the whole proof: they are said to be ‘discharged.’

** The rules of truth and absurdity

\begin{equation}
  \begin{prooftree}
    \hypo{P}
    \rewrite{}
    \infer1[$\top_{\text{I}}$]{\top}
  \end{prooftree}
  \qquad\text{and}\qquad
  \begin{prooftree}
    \hypo{\bot}
    \infer1[$\bot_{\text{E}}$]{P}
  \end{prooftree}.
\end{equation}

The first is by way of an axiom: one is allowed to assume $\top$ starting from
nothing. In the second note that the conclusion is an arbitrary proposition:
‘from a false proposition one can prove any proposition.’

** Example

Let’s prove that $\wedge$ distributes over $\vee$. We shall do this by assuming
$(P\wedge Q)\vee(P\wedge R)$ as a premise and obtaining $P\wedge (Q\vee R)$ as a
conclusion. (The converse will turn out not to have a proof, however.)
o
The proof is too large to reproduce across the page. The first part is:
 \begin{equation*}
   \begin{prooftree}
     \hypo{(P \wedge Q) \vee (P \wedge R)}
     \hypo{[P \wedge Q]}
     \infer1[$\wedge_{\text{E1}}$]{P}
     \hypo{[P \wedge R]}
     \infer1[$\wedge_{\text{E1}}$]{P}    
     \infer3[$\vee_{\text{E}}$]{P}
   \end{prooftree}.
 \end{equation*}

The second part is:
 \begin{equation*}
   \begin{prooftree}
     \hypo{(P \wedge Q) \vee (P \wedge R)}
     \hypo{[P \wedge Q]}
     \infer1[$\wedge_{\text{E1}}$]{Q}
     \infer1[$\vee_{\text{I1}}$]{Q \vee R}
     \hypo{[P \wedge R]}
     \infer1[$\wedge_{\text{E1}}$]{R}
     \infer1[$\vee_{\text{I2}}$]{Q \vee R}
     \infer3[$\vee_{\text{E}}$]{Q \vee R}
   \end{prooftree}.
 \end{equation*}

And putting the two together, where one imagines replacing the first ellipsis
with the first part and the second ellipsis with the second part:
 \begin{equation*}
   \begin{prooftree}
     \hypo{(P \wedge Q) \vee (P \wedge R)}
     \ellipsis{1}{P}
     \hypo{(P \wedge Q) \vee (P \wedge R)}
     \ellipsis{2}{Q \vee R}
     \infer2[$\wedge_{\text{I}}$]{P \wedge (Q \vee R)}
   \end{prooftree}.
 \end{equation*}

What is perhaps surprising is that the converse is /not/ provable. The
conclusion of the converse, $(P\wedge Q)\vee(P\wedge R)$, has the form of a
disjunction, and the only way to conclude a disjunction, with the rules as
given, is by disjunction introduction; and that requires a proof of one of the
disjuncts. By symmetry, a proof of one of the disjuncts would be a proof of the
other, /mutatis mutandis/. But no such proof can exist. For, if proofs of both
disjunct existed, they would be a valid classical proofs; but in classical logic
it is entirely possible that one of the disjuncts is false yet the disjunct is
true.

Since we can't prove something that, under the classical interpretation, is
true, this little logic $(\wedge, \vee, \top, \bot)$ is /incomplete/ with
respect to the classical interpretation. So one interesting question is, is
there an interpretation for which it is complete? I'll return to this in a later
section.

** Comments

This whole natural deduction business is to me rather unsatisfactory. The
philosophy seems to be that to assert a proposition means that one has a proof
of it; yet the only things that look like proofs are these trees of rules. Are
they supposed to be the same thing as propositions? In which case, why aren't
they written the same? Or are there really two kinds of things that are proofs?

Furthermore, the example proof given above cannot itself be a proposition
because it has undischarged premises (namely, the premise!) and the logic we
have doesn't include implication as a possible kind of proposition. Worse yet,
there are no theorems apart from $\top$. A theorem is a proposition that is the
conclusion of a proof with no premises, and the only way to proceed from no
premises is by $\top_\text{I}$.

At this stage, your standard textbook will introduce the ‘sequent calculus.’ The
following is not, as far as I can tell, the standard sequent calculus. Except,
it is likely that I do not understand the standard sequent calculus, so maybe it
is.


* ‘Sequents’

The basic idea is to retreat from the philosophy that a proposition is the set
of its proofs, and to deal directly with things that are rather like proofs. /As
it happens/, in the full version of all of this, it /is/ possible to identify
sets of proofs with propositions; but that doesn't mean they are the same thing.

A /judgement/ is a pair of propositions, written $\phi\vdash\psi$, and is
/interpreted/ to mean that, given a proof of $\phi$, there exists a proof of
$\psi$. The plan is to write down some axiomatic judgements and some ways of
constructing new judgements given existing ones: we identify a judgement with
its construction. Finally, for reasons to be explained later, we introduce an
equivalence on judgments, and the equivalence classes will be the real objects
of study.

Note that---as far as I can tell---this is not what is usually done (in
Gentzen's sequent calculus). What is usually done is that the thing on the left
of the judgement turnstile is a multiset of premises (and in the classical
version the thing on the right is also a multiset as well). But I am trying to
understand how all this is related to categories; and thus I am looking for a
morphism. I don't feel too bad about this adjustment to nomenclature since it
appears that everyone in this field is happy to reinvent the game as well.

** Proofs

Judgements are constructed according to proofs. A proof is a connected series of
judgements, each following from the previous according to certain rules. The
rules of proofs are as follows.

By the way, I am assuming that there are no sequents of the form $A\vdash B$,
where $A$ and $B$ are atomic propsitions, other than $A\vdash A$. 

** Axiomatic judgements

*** Identity

For any proposition $\phi$,
\begin{equation}
\begin{prooftree}
\infer0[id]{\phi \vdash \phi}
\end{prooftree}.
\end{equation}

*** Conjunction

For any propositions $\phi$, and $\psi$,
\begin{equation}
  \begin{prooftree}
    \infer0[$\pi_1$]{\phi \wedge \psi \vdash \phi}
  \end{prooftree}
  \qquad\text{and}\qquad
  \begin{prooftree}
    \infer0[$\pi_2$]{\phi \wedge \psi \vdash \psi}
  \end{prooftree}.
\end{equation}

*** Disjunction

For propositions $\phi$, and $\psi$,
\begin{equation}
  \begin{prooftree}
    \infer0[$\iota_1$]{\phi \vdash \phi \vee \psi}
  \end{prooftree}
  \qquad\text{and}\qquad
  \begin{prooftree}
    \infer0[$\iota_2$]{\psi \vdash \phi \vee \psi}
  \end{prooftree}.
 \end{equation}

*** Truth and falsity

\begin{equation}
  \begin{prooftree}
    \infer0[unit]{\phi \vdash \top}
  \end{prooftree}
  \qquad\text{and}\qquad
  \begin{prooftree}
    \infer0[]{\bot \vdash \psi}
  \end{prooftree}.
 \end{equation}

** Rules of combination

As well as the axioms, we may also produce new judgements from existing
ones. The rules are as follows.

*** Conjunction

For any propositions $P$, $\phi$, and $\psi$, and judgements $P \vdash \phi$ and
$P \vdash \psi$,
\begin{equation}
  \begin{prooftree}
    \hypo{P \vdash \phi}
    \hypo{P \vdash \psi}
    \infer2[$\times$]{P \vdash \phi\wedge\psi}
  \end{prooftree}.
 \end{equation}

*** Disjunction

For any propositions $P$, $\phi$, and $\psi$, and judgements $\phi \vdash P$ and
$\psi \vdash P$, 
\begin{equation}
  \begin{prooftree}
    \hypo{\phi \vdash P}
    \hypo{\psi \vdash P}
    \infer2[$+$]{\phi \vee \psi \vdash P}
  \end{prooftree}.
 \end{equation}

** Rule of composition

For propositions $P$, $Q$, and $R$, and judgements $P\vdash Q$ and $Q\vdash R$,
\begin{equation}
  \begin{prooftree}
    \hypo{P \vdash Q} \hypo{Q \vdash R}
    \infer2[$\circ$]{P \vdash R}
  \end{prooftree}.
\end{equation}

** Example

Let's try to construct the judgement $(P\wedge Q)\vee(P\wedge R) \vdash P\wedge
(Q\vee R)$. Again, the whole proof is somewhat too large to fit. We need to
complete the following:
\begin{equation*}
  \begin{prooftree}
    \hypo{}\ellipsis{1}{(P \wedge Q) \vee (P \wedge R) \vdash P}
    \hypo{}\ellipsis{2}{(P \wedge Q) \vee (P \wedge R) \vdash (Q \vee R)}
    \infer2[$\times$]{(P \wedge Q) \vee (P \wedge R) \vdash P \wedge (Q \vee R)}
  \end{prooftree}
\end{equation*}
The first part (1) is straightforward:
\begin{equation*}
  \begin{prooftree}
    \infer0[$\pi_1$]{P \wedge Q \vdash P}\infer0[$\pi_1$]{P \wedge R \vdash P}
    \infer2[$+$]{(P \wedge Q) \vee (P \wedge R) \vdash P}
  \end{prooftree}
\end{equation*}
But the second part (2) involves composition/cut:
\begin{equation*}
  \begin{prooftree}
        \infer0[$\pi_1$]{P \wedge Q \vdash Q}\infer0[$\iota_1$]{Q \vdash Q \wedge R}
      \infer2[$\circ$]{P \wedge Q \vdash Q \vee R}
        \infer0[$\pi_1$]{P \wedge R \vdash R}\infer0[$\iota_2$]{R \vdash Q \vee R}
      \infer2[$\circ$]{P \wedge R \vdash Q \vee R}
    \infer2[$+$]{(P \wedge Q) \vee (P \wedge R) \vdash (Q \vee R)}
  \end{prooftree}
\end{equation*}

If both $P \vdash Q$ and $Q \vdash P$ then we write $P = Q$. As another example,
we show $A \wedge A = A$ (which certainly seems plausible but you never know
with this intuitionistic stuff).

First it is immediate from $\pi_1$ that $A \wedge A \vdash A$. The converse is
nearly as straightforward:
\begin{equation}
  \begin{prooftree}
    \infer0[id]{A \vdash A}
    \infer0[id]{A \vdash A}
    \infer2[$\times$]{A \vdash A \wedge A}
  \end{prooftree}
\end{equation}

Likewise, $A \vee A = A$, by a similar construction. Some other examples are:
$\top\wedge A = A$; $\bot\wedge A = \bot$; $\top\vee A = \top$; and $\bot\vee A
= A$. For example, a construction of the first is:
\begin{equation}
  \begin{prooftree}
    \infer0[$\pi_2$]{\top \wedge A \vdash A}
  \end{prooftree}
  \qquad\text{and}\qquad
  \begin{prooftree}
    \infer0[unit]{A \vdash \top}
    \infer0[id]{A \vdash A}
    \infer2[$\times$]{A \vdash \top \wedge A}
  \end{prooftree}
\end{equation}


** The ‘cut rule’

The rule of composition, in the traditional presentation, is known as the ‘cut
rule.’ It is disliked because unlike the other rules it has a proposition in the
premise, $Q$, which does not occur in the conclusion. If one is trying to write
an automated theorem prover this is something of a problem. An automated theorem
prover will search backwards from conclusions: if the cut is allowed then the
search space for premises is unbounded.

There is a thereom, called ‘Hauptsatz,’ or ‘cut elimination,’ to the effect that
any construction involving cut can be replaced by one not involving cut. Clearly
that is not the case with the rules as written above. 

For example, consider the construction of $P\wedge Q\vdash R$ from $P\vdash
R$. It seems like the sort of step we would want to make: surely, if we can
conclude $R$ given $P$, then we can conclude $R$ given $P\wedge Q$? And indeed,
the rule of composition permits this construction:
 \begin{equation*}
  \begin{prooftree}
    \infer0[$\pi_1$]{P \wedge Q \vdash P}
    \hypo{}
    \ellipsis{}{P \vdash R}
    \ellipsis{}{}
    \infer2[$\circ$]{P \wedge Q \vdash R} 
  \end{prooftree}
\end{equation*}
In the absence of the rule of construction, the last judgement is not of the
form of the conclusion of any rule: the rule `$\times$' has $\wedge$ on the
right, whereas the conclusion above has $\wedge$ on the left; and the axioms of
conjunction have one symbol the same on both sides of the judgement, whereas the
conclusion above has no symbols in common between the two side. So the rule of
composition is required unless we introduce new rules.

One thing we might do is the following. The rule of composition allows us to
construct new judgements from existing ones. Perhaps we can just write down
these new judgements as rules. For example, given that the construction of
propositions introduces $\phi\wedge\psi$, we might ask what judgements,
introduced by the rule of composition, have $\phi\wedge\psi$ on their left- or
right-hand side. That is, can we fill in the following judgements:
\begin{equation*}
  \begin{prooftree}
    \hypo{P \wedge Q \vdash \text?}
    \hypo{\text?}
    \infer2{\text?}
  \end{prooftree},\quad
  \begin{prooftree}
    \hypo{\text? \vdash P \wedge Q}
    \hypo{\text?}
    \infer2{\text?}
  \end{prooftree},\quad
  \begin{prooftree}
    \hypo{\text?}
    \infer1{P \wedge Q \vdash \text?}
  \end{prooftree},\quad\text{and}\quad
  \begin{prooftree}
    \hypo{P \wedge Q \vdash \text?}
    \hypo{\text?}
    \infer2{\text? \vdash P \wedge Q}
  \end{prooftree}.
\end{equation*}

Presumably, they would look something like this (where we've already got the
last one as the rule of conjunction):
\begin{equation*}
  \begin{prooftree}
    \hypo{\Gamma \vdash P}
    \hypo{\Gamma \vdash Q}
    \hypo{P \wedge Q \vdash R}
    \infer3{\Gamma \vdash R}
  \end{prooftree},\qquad
  \begin{prooftree}
    \hypo{R \vdash P \wedge Q}
    \infer1{R \vdash P}
  \end{prooftree},\qquad
  \begin{prooftree}
    \hypo{R \vdash P \wedge Q}
     \infer1{R \vdash Q}
  \end{prooftree},
\end{equation*}
and
\begin{equation*}
  \begin{prooftree}
    \hypo{P \vdash \Gamma}
    \infer1{P \wedge Q \vdash \Gamma}
  \end{prooftree},\qquad
  \begin{prooftree}
    \hypo{Q \vdash \Gamma}
    \infer1{P \wedge Q \vdash \Gamma}
  \end{prooftree},\quad\text{and}\quad
  \begin{prooftree}
    \hypo{\Gamma \vdash P}
    \hypo{\Gamma \vdash Q}
    \infer2[$\times$]{\Gamma \vdash P \wedge Q}
  \end{prooftree}.
\end{equation*}
Note that these constructions come in two kinds: those that “introduce” a
conjunction (either on the left or the right of the judgement) and those that
“eliminate” one (either on the left or the right).

** Composition (again)

Conversely, let's apply the rule of composition to as many things as possible
(still sticking with conjuction only). The two parts of the premise of $\circ$
could come from either the axiom or the rule of composition. 

The conclusion of the axiom has the form $\phi\wedge\psi \vdash \phi$ (or $\psi$
on the right) and the conclusion of the rule has the form $P\vdash
\phi\wedge\psi$, where $P\vdash\phi$ and $P\vdash\psi$. If we use composition to
cut out the product, we get a judgement $P\vdash\phi$: 
\begin{equation*}
  \begin{prooftree}
    \hypo{P \vdash \phi}
    \hypo{P \vdash \psi}
    \infer2[$\times$]{P \vdash \phi \wedge \psi}
    \infer0[$\pi_1$]{\phi \wedge \psi \vdash \phi}
    \infer2[$\circ$]{P \vdash \phi}
\end{prooftree}.
\end{equation*}
Note that the judgement $P\vdash\phi$ in the premise is not necessarily the same
judgement as the judgement $P\vdash\phi$ in the conclusion. We identify
judgements by their construction and these two have /prima facie/ different
constructions.

We could also try composition the other way around:
\begin{equation*}
  \begin{prooftree}
    \infer0[$\pi_1$]{\phi \wedge \psi \vdash \phi}
    \hypo{\phi \vdash P}
    \hypo{\phi \vdash Q}
    \infer2[$\times$]{\phi \vdash P \wedge Q}
    \infer2[$\circ$]{\phi \wedge \psi \vdash P \wedge Q}
\end{prooftree}.
\end{equation*}

We could apply composition to two axioms:
\begin{equation*}
  \begin{prooftree}
    \infer0[$\pi_1$]{(\phi \wedge \psi) \wedge \rho \vdash \phi \wedge \psi}
    \infer0[$\pi_1$]{\phi \wedge \psi \vdash \phi}
    \infer2[$\circ$]{(\phi \wedge \psi) \wedge \rho \vdash \phi}
\end{prooftree}.
\end{equation*}

And finally, we could apply composition to two rules of conjunction:
\begin{equation*}
  \begin{prooftree}
    \hypo{\phi \vdash P}
    \hypo{\phi \vdash Q}
    \infer2[$\times$]{\phi \vdash P \wedge Q}

    \hypo{P \wedge Q \vdash R}
    \hypo{P \wedge Q \vdash S}
    \infer2[$\times$]{P \wedge Q \vdash R \wedge S}
    \infer2[$\circ$]{\phi \wedge \psi \vdash R \wedge S}
  \end{prooftree}.
\end{equation*}

 

* Categories

If one is familiar with category theory, one would suspect that $\wedge$ is a
product (and $\vee$ a coproduct). The axioms $\pi_1$ and $\pi_2$ look a lot like
the the projection operators that occur in the definition of a product; and the
rule $\times$ looks a lot like the mediating morphism. However, there is
something missing. In the categorical definitions, the mediating morphism
doesn't just exist, it forms a commuting diagram and is unique in doing so. The
commuting condition says that, given $P\vdash\phi$ and $P\vdash\psi$, then:
\begin{equation*}
    P \vdash \phi
\quad = \quad
  \begin{prooftree}
    \hypo{P \vdash \phi}
    \hypo{P \vdash \psi}
    \infer2[$\times$]{P \vdash \phi \wedge \psi}
    \infer0[$\pi_1$]{\phi \wedge \psi \vdash \phi}
    \infer2[$\circ$]{P \vdash \phi}
  \end{prooftree}.
\end{equation*}
Note that the above is saying that two /prima facie/ different judgements are in
fact the same. They are different /prima facie/ because their constructions are
different.


* Programs

Here is what you might call a “programming language,” if you were being
charitable.

Fix, once and for all, a countable collection of /atomic types/,
$A, B, C, \dotsc$. A /type/ is either an atomic type, or $\top$, or $P\times Q$,
where $P$ and $Q$ are types. (In code, we write ~P * Q~ for $P\times Q$.) An
/arrow type/ is a pair of types separated by an arrow, such as $P\to Q$. If
$P\to Q$ is an arrow type, we call $P$ the /domain/ and $Q$ the /range/.

A program is now “a thing with an arrow type.” The syntax of programs is defined
by terms and expressions. A /term/ is one of the following, where $e_1$ and
$e_2$ are arbitrary expressions (which will be defined in a minute):
\begin{equation*}
\begin{align}
  t \equiv \unit \mid \id \mid \fst \mid \snd \mid (e_1, e_2) \mid e_2 \circ e_1.
\end{align}
\end{equation*}
Note that the intended reading of the last rule, involving a juxtaposition of
two expressions, is the composition of two functions, the the second following
the first. (But note that there are no functions here.)

An /expression/ is defined by a term together with an arrow type, which we write
as $t : P\to Q$. Not all combinations of terms and arrow types are expresssions:
the following defines which constructions are:
\begin{equation*}
\begin{align}
  e \equiv &\unit : P \to \top \\
    &\mid \id : P \to P \\
    &\mid \fst : P \times Q \to P \\
    &\mid \snd : P \times Q \to Q \\
    &\mid (t_1 : P \to Q, t_2 : P \to R) : P \to Q \times R \\
    &\mid (t_2 : Q \to R) \circ (t_1 : P \to Q) : P \to R.
\end{align}
\end{equation*}
You can see that the rules say how to line up the types to make sensible
combinations. For example, the rule for $\id$ says that “identities map a type
to itself.” (Not really, because terms aren't maps and types aren't sets; but
that's the sort of thing one is supposed to imagine.) As another example, the
rule for composition says that the range of $e_1$ has to be the domain of $e_2$,
which is very sensible if you want to compose functions.

It is sometimes difficult to write out expressions because they rapidly become
very long. An alternative approach is to write the syntax tree of the expression
as an actual tree. Each node in the tree is an expression with the types erased,
except for the type of the whole expression; the children of that node
demonstrate that it is an allowed expression by exhibiting the types of its
constituents.

For example, consider the following expression
\begin{equation*}
\bigl(\fst : A \times \top \to A\bigr) \circ \bigl( (\id :A \to A, \unit : A \to \top) : A \to A \times \top\bigr) : A \to A.
\end{equation*}
Here is the same thing written in the tree notation. Note that parent nodes are
written below their child nodes; so we start at the top with the innermost
(atomic) expressions.
\begin{equation*}
  \begin{prooftree}
    \hypo{\fst : A \times \top \to A}
    \hypo{\id : A \to A}
    \hypo{\unit : A \to \top}
    \infer2{(\id, \unit) : A \to A \times \top}
    \infer2{\fst \circ (\id, \unit) : A \to A}
  \end{prooftree}
\end{equation*} 

The tree notation conveys the same information as the expression but is easier
to read. In particular, it is easier to check that the types are all of the
correct form for each way of construction an expression.

Well, now, it /almost/ looks like we have a category. The definition would go
something like this: let the objects be types, and let the morphisms be
expressions. A morphism $e:P\to Q$ is just an expression whose arrow type is
$P\to Q$. To compose morphisms, use the rule of composition.

However, we do not, thereby, obtain a category. One must check the category
axioms and these in general do not hold. For example, the only reasonable
candidate for an identity morphism is $\id$ but the expression $\id:A\to A$
is patently not the same expression as $(\id:A\to A)\circ(\id:A\to A)$.

A separate problem is that composition (of expressions) is not
associative. Consider the following expression (written in tree notation):
\begin{equation*}\footnotesize
  \begin{prooftree}
    \hypo{\snd : A \times B \to B}
    \hypo{\fst : (A \times B) \times C \to A \times B}
    \hypo{\fst : ((A \times B) \times C) \times D \to (A \times B) \times C}
    \infer2{\fst\circ\fst : ((A \times B) \times C) \times D \to A \times B}
    \infer2{\snd\circ(\fst\circ\fst) : ((A \times B) \times C) \times D \to B}
    \end{prooftree}.   
\end{equation*}
And here is another way of constructing the same expression:
\begin{equation*}\footnotesize
  \begin{prooftree}
    \hypo{\snd : A \times B \to B}
    \hypo{\fst : (A \times B) \times C \to A \times B}
    \infer2{\snd\circ\fst : (A \times B) \times C \to B}
    \hypo{\fst : ((A \times B) \times C) \times D \to (A \times B) \times C}
    \infer2{(\snd\circ\fst)\circ\fst : ((A \times B) \times C) \times D \to B}
    \end{prooftree}.   
\end{equation*}
On the face of it, these two are not the same construction, hence not the same
expression. 

It turns out that we can fix all these problems by introducing an equivalence
relation on well-typed programs. First, we simply don't indicate the order of
composition in terms. For example, we write ~snd fst fst~ rather than ~(snd fst)
fst~ or ~snd (fst fst)~.

Second, we introduce a reduction relation on terms, with the following two
rules:
\begin{equation*}
  t\; \text{\tt id} \Rightarrow t
  \quad\text{and}\quad
  \text{\tt id}\; t \Rightarrow t 
\end{equation*}
for any term $t$;
\begin{equation*}
  \text{\tt unit}\; t \Rightarrow \text{\tt unit}
\end{equation*}
for any term $t$; and 
\begin{equation*}
  \text{\tt fst}\;(t_1, t_2) \Rightarrow t_1 
  \qquad\text{and}\qquad
  \text{\tt snd}\;(t_1, t_2) \Rightarrow t_2 
\end{equation*}
for any terms $t_1$ and $t_2$. (Note that these relations on terms are all
arrow type--preserving and so are also reduction relations on expressions.) 


* OLD

We note that, if some expression has a well-typed program, then it is always
possible to deduce it. (A separate question, not addressed here, is whether one
can always assign an arrow type to a term to produce a well-typed program.)

To see this, consider an expression $e = t:P\to Q$, where $t$ is a term and $P$
and $Q$ are types. If $t$ is one of the “built-in” terms, ~fst~, ~snd~, ~id~, or
~unit~, then $P$ and $Q$ must have the appropriate forms for those types and we
are done. Otherwise, $t$ is either a product or a composition. If it is a
product, then we must have $t = (r, s)$ for some $r$ and $s$ and, in addition,
$Q = R\times S$ for some $R$ and $S$. We may therefore assign arrow types to $r$
and $s$ as $r : P\to R$ and $s : P\to S$. 

On the other hand, if $t$ is a composition, then by associativity, it must be of
the form 














 




* Footnotes

[fn:1] Jean-Yves Girard, /Proofs and Types/ (available from
http://paultaylor.eu/stable/prot.pdf).  
